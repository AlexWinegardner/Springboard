{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e788fa7",
   "metadata": {},
   "source": [
    "# Gravitational Wave Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "555a5474",
   "metadata": {},
   "source": [
    "We will begin by importing data from The Gravitational-wave Transient Catalog (GWTC): https://gwosc.org/eventapi/html/GWTC/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e86e7778",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "gw_data = pd.read_csv('GWTC.csv')\n",
    "gw_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d189ab04",
   "metadata": {},
   "outputs": [],
   "source": [
    "gw_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d743175",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We want rows that contain only confidently detected events \n",
    "\n",
    "confident_data = gw_data[gw_data['catalog.shortName'].str.contains('confident')].reset_index(drop = True)\n",
    "confident_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6864ef53",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing irrelevant data – to simplify the analysis we can also ignore upper and lower errors for now\n",
    "\n",
    "relevant_data = confident_data.drop(columns = ['id', 'version', 'reference', 'jsonurl', 'mass_1_source_lower', \n",
    "                                               'mass_1_source_upper', 'mass_2_source_lower', 'mass_2_source_upper',\n",
    "                                               'network_matched_filter_snr_lower', 'network_matched_filter_snr_upper',\n",
    "                                               'luminosity_distance_lower', 'luminosity_distance_upper',\n",
    "                                               'chi_eff_lower', 'chi_eff_upper', 'total_mass_source_lower',\n",
    "                                               'total_mass_source_upper', 'chirp_mass_source_lower', \n",
    "                                               'chirp_mass_source_upper', 'chirp_mass', 'chirp_mass_lower',\n",
    "                                               'chirp_mass_upper', 'redshift_lower', 'redshift_upper', 'far_lower',\n",
    "                                               'far_upper', 'p_astro_lower', 'p_astro_upper', 'final_mass_source_lower',\n",
    "                                               'final_mass_source_upper', 'far'\n",
    "                                               ])\n",
    "relevant_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20541b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inconsistent and missing values for \"total_mass_source\" – we will drop this column and add a corrected one\n",
    "\n",
    "df = relevant_data.drop(columns = 'total_mass_source')\n",
    "\n",
    "df.insert(5, 'total_mass_source', df['mass_1_source'] + df['mass_2_source'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "407c3622",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We will reorder \"final_mass_source\" to better organize the data\n",
    "\n",
    "column_names = df.columns.tolist()\n",
    "last_column = column_names[-1]\n",
    "column_names.remove(last_column)\n",
    "column_names.insert(6, last_column)\n",
    "\n",
    "df = df[column_names]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e2bda6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create new column that measures the radiated energy from the merger event\n",
    "\n",
    "df.insert(7, 'radiated_energy', df['total_mass_source'] - df['final_mass_source'])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69851563",
   "metadata": {},
   "source": [
    "It seems we have just spotted something interesting in the data!\n",
    "\n",
    "First, we notice that the radiated energy for GW150914 is 3.1 (solar masses * c^2), which is exactly in agreement with the published results from the Ligo/Virgo collaboration: https://en.wikipedia.org/wiki/First_observation_of_gravitational_waves\n",
    "\n",
    "Second, we notice that the radiated energy for GW200322_091133 is negative! We know from conservation of energy that this is an impossible result. The data seems to be implying that the resulting black hole gained an extra 5 solar masses out of nowhere! \n",
    "\n",
    "We must investigate further to understand the reasons for this descrepency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1516e0f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets find all the rows with negative values for radiated energy\n",
    "\n",
    "negative_energy = df[df['radiated_energy'] < 0]\n",
    "negative_energy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db0f5a1",
   "metadata": {},
   "source": [
    "Seems like only a couple of events have this issue, and GW170817 by only a small amount (-0.07). By further investigating the publications on GW170817 (https://en.wikipedia.org/wiki/GW170817), we can see that it was the first detection of a neutron star binary merger (hence why the masses are so small in comparison to other events). \n",
    "\n",
    "We can reason that the discrepency for negative radiated energy can be attributed to the errors in the measurments for the source masses – which means that now might be a good time to investigate how large these errors can be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d9a5f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "errors = confident_data[['commonName','mass_1_source_lower', 'mass_1_source_upper', 'mass_2_source_lower', 'mass_2_source_upper']]\n",
    "errors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40570c07",
   "metadata": {},
   "source": [
    "From simple inspection we can see that some events such as GW170608 have relatively low upper and lower erros, but GW200322_091133 (the event we found with negative radiated energy) has a whopping upper error margin of 48 solar masses! This explains why we got a negative value – there is currently too much uncertainty in the measurements for this event. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32e00b61",
   "metadata": {},
   "source": [
    "Another important column we can look at is \"p_astro\" – which tells us the probability that the detection came from an actual astrophysical event:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "947ca392",
   "metadata": {},
   "outputs": [],
   "source": [
    "astro_prob = df[['commonName','p_astro']]\n",
    "astro_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a722c4d9",
   "metadata": {},
   "source": [
    "Again, we can see that GW200322_091133 stands out with a low probability of 61% in comparison to other events. Let's see how many events remain if we are more selective and filter for 80% probability:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04662f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['p_astro'] >= 0.8].reset_index(drop = True)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59cc8ff5",
   "metadata": {},
   "source": [
    "We retain 74 events, which is not too bad. At least now we have a much higher degree of confidence in the accuracy of the features that we will later be working with to develop our machine learning model. \n",
    "\n",
    "Next, one last step is that we want to make sure we are only working with black holes. For the purposes of this project, we don't want to add any unecessary complexities that neutron stars might introduce.\n",
    "\n",
    "To accomplish this, we will filter out any event that has any mass below 3 solar masses (the Tolman–Oppenheimer–Volkoff limit for neutron stars is ~2.1 solar masses, but we will be a bit conservative because of the error bars)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e4a0c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[(df['mass_1_source'] >= 3) & (df['mass_2_source'] >= 3)].reset_index(drop = True)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd5f3b5f",
   "metadata": {},
   "source": [
    "This gives us our final collection of events that we will analyze. Each individual event has access to 32s of data around the time of merger which we will then use to builld our model. \n",
    "\n",
    "We will begin by creating a list of all our events for us to iterate over once we begin downloading the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be17c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "events = [i for i in df['commonName']]\n",
    "events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea874240",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(events)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4896f711",
   "metadata": {},
   "source": [
    "For now we will just import data from GW150914, and then later we will generalize the procedure for all other events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78f729f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gwosc import datasets\n",
    "from gwosc.datasets import find_datasets\n",
    "\n",
    "GW150914 = datasets.find_datasets(type='events', catalog='GWTC-1-confident')\n",
    "print('First Black Hole Merger:', GW150914[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be724fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gwosc.locate import get_event_urls\n",
    "\n",
    "url_list = get_event_urls('GW150914', duration=32, detector='H1')\n",
    "print(url_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf78e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import urllib.request\n",
    "\n",
    "def download(url):\n",
    "    filename = url.split('/')[-1]\n",
    "    print('Downloading ' + url )\n",
    "    urllib.request.urlretrieve(url, filename)  \n",
    "    print(\"File download complete\")\n",
    "    return filename\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f8aa83",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = url_list[0]\n",
    "filename = url.split('/')[-1]\n",
    "download(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4b6afe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "\n",
    "# Load file: data\n",
    "data = h5py.File(filename, 'r')\n",
    "\n",
    "# Print the datatype of the loaded file\n",
    "print(type(filename))\n",
    "\n",
    "# Print the keys of the file\n",
    "for key in data.keys():\n",
    "    print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c30841f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# Get the HDF5 group: group\n",
    "group = data['strain']\n",
    "\n",
    "# Check out keys of group\n",
    "for key in group.keys():\n",
    "    print(key)\n",
    "\n",
    "# Set variable equal to time series data: strain\n",
    "strain = np.array(data['strain']['Strain'])\n",
    "\n",
    "# Set number of time points to sample: num_samples\n",
    "num_samples = 10000\n",
    "\n",
    "# Set time vector\n",
    "time = np.arange(0, 1, 1/num_samples)\n",
    "\n",
    "# Plot data\n",
    "plt.plot(time, strain[:num_samples])\n",
    "plt.xlabel('GPS Time (s)')\n",
    "plt.ylabel('strain')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3b46751",
   "metadata": {},
   "source": [
    "Now we will go over the generalized procedure for importing all the data in our entire events list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d3a910",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_urls = []\n",
    "\n",
    "for event in events:\n",
    "    url_list = get_event_urls(event, duration=32, detector='H1')\n",
    "    \n",
    "    # If no URL for detector 'H1', get the URL for detector 'L1'\n",
    "    if not url_list:\n",
    "        url_list = get_event_urls(event, duration=32, detector='L1')\n",
    "    \n",
    "    all_urls.extend(url_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7418fcba",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, 70):\n",
    "    url = all_urls[i]\n",
    "    filename = url.split('/')[-1]\n",
    "    download(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b56a809",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae58a785",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (igwn-py39)",
   "language": "python",
   "name": "igwn-py39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
